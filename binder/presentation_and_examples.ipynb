{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc77dff",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gpflow\n",
    "import xarray as xr\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from shgp import SHGP\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'jupyterlab'\n",
    "import plotly\n",
    "from tqdm import trange\n",
    "from helper_funs import add_plot, plot_clim, plot_fig3, fit_fig3, plot_fig5, fit_fig5\n",
    "\n",
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# Seeding for reproducibility\n",
    "SEED = 30\n",
    "rng = np.random.RandomState(SEED)\n",
    "tfp_seed = tfp.random.sanitize_seed(SEED)\n",
    "\n",
    "# Plotly specifics\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "pio.templates[\"pres\"] = go.layout.Template(\n",
    "    layout=go.Layout(\n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        plot_bgcolor='rgba(0,0,0,0)',\n",
    "        colorway=px.colors.qualitative.Set2,\n",
    "        legend=dict(itemsizing='trace', font_size=26),\n",
    "        font_size=26,\\\n",
    "        \n",
    "    )\n",
    ")\n",
    "\n",
    "config= dict(displayModeBar=False)\n",
    "\n",
    "# Set plotly defaults\n",
    "pio.templates.default = 'none+pres'\n",
    "full_fig_width = 1000\n",
    "full_fig_height = 600\n",
    "half_fig_width = full_fig_width // 2\n",
    "half_fig_height = full_fig_height // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183dcd2b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Estimating latent processes from sets of vectors with sparse hierarchical Gaussian processes\n",
    "\n",
    "## Identifying the underlying climate signals from multiple climate models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9671e0",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Dry title but I wanted to highlight that this work was about both methodological advancements and an important application.\n",
    "\n",
    "Work done alongside DSNE PhD Tom Pinder, which forms part of a larger effort to construct climate projections from multiple climate models using Bayesian methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794fc4aa",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Say we had output from 5 different climate models which estimate the global mean surface temperature from 1960-2014. \n",
    "\n",
    "We would expect that all the models have a similar latent functionality which contains to smooth increase in temperature as the impact of anthropogenic emissions kicks in, coupled with the annual oscillation we see across all parts of the earth. But on top of that, because we are modelling such a chaotic system, each climate model has its own variability caused by meteorology and other subdecadal effects which creates the difference at in models across time.\n",
    "\n",
    "I'm going to show you how we can model data like this in order to extract the latent function whilst making robust uncertainty estimates. I'll talk through this climate example, but bear in mind that we could also apply this method to problems such as modelling the latent air quality of a city from numerous sensors, or any time we want to model a hierarchical system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3671a49",
   "metadata": {
    "cell_style": "center",
    "hide_input": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "da = xr.open_dataarray('./../data/climate_model_data_1D_GMST.nc')\n",
    "plot_clim(da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d276578",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Hierarchical model\n",
    "$$\\mathbf{Y} = \\{\\mathbf{y}_1, \\mathbf{y}_2, \\mathbf{y}_3\\}$$\n",
    "\n",
    "$$\\mathbf{y}_i = f_i(\\mathbf{x}) + g(\\mathbf{x})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2b0bb0",
   "metadata": {
    "cell_style": "center",
    "hide_input": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Hide, Run initially\n",
    "fig2 = go.FigureWidget()\n",
    "fig2.update_layout(yaxis={'visible': False, 'showticklabels': False})\n",
    "fig2.update_layout(xaxis={'visible': False, 'showticklabels': False})\n",
    "\n",
    "fig2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a28f565",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The way we want to model this data is hierarchically. Say for example we suppose that some times series y all share some latent functionality g, in addition to their individual functionality f_i.\n",
    "\n",
    "You can see that here. We've defined the shared funcionality g and modulated it to create three different time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1931a31b",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "g = np.sin(2 * x) + np.cos(1 * x)\n",
    "add_plot(x, g, 'g(x)', fig2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad18090",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y1 = g + np.sin(3 * x)\n",
    "add_plot(x, y1, 'y1', fig2)\n",
    "y2 = g + x * 0.2\n",
    "add_plot(x, y2, 'y2', fig2)\n",
    "y3 = g + 1\n",
    "add_plot(x, y3, 'y3', fig2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a85905e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\\begin{align}\n",
    "g(\\mathbf{x}) &\\sim \\mathcal{GP}(\\mathbf{0}, k_g(\\mathbf{x}, \\mathbf{x}')) \\\\\n",
    "f^{(i)}(\\mathbf{x}) &\\sim g(\\mathbf{x}) + \\mathcal{GP}(\\mathbf{0}, k^{(i)}(\\mathbf{x}, \\mathbf{x}')) \\\\\n",
    "f^{(i)}(\\mathbf{x}) &\\sim \\mathcal{GP}(\\mathbf{0}, k^{(i)}(\\mathbf{x}, \\mathbf{x}') + k_g(\\mathbf{x}, \\mathbf{x}'))\\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35b738c",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A sensible approach would be to model this using a hierarchical set of Gaussian processes as they allow us to robustly propogate uncertainty through the whole hierarchical model. We've defined the group function as a Gaussian process with a covariance function defined by the kernel kg. As we can easily add Gaussian processes together we can add group functionality to individual Gaussian processes that model the additional functionality related to the individual signals. \n",
    "\n",
    "So in summary, we model the individual time series using a Gaussian process, using two covariance functions which seperately describe the individual behaviour and the behaviour that is shared across all time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bc0afd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# BUT!\n",
    "\n",
    "$${\\Huge \\mathcal{O}(n^3r^2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e8d9de",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The issue with this model is it doesn't scale well. Like normal Gaussian processes it scales cubically with the size of the data as well as quadratically with the number of individual time series, which for the above case was three.\n",
    "\n",
    "So to realistically apply this model environmnetal dataset which are typically fairly large we need to find a way to speed that up. So we use variational approaches from the Gaussian process literature to make this hierarchical model usable. I won't go into details but I'll hopefully provide some inuition as to how it works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aac5a8f",
   "metadata": {
    "hide_input": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig3, gp, sgp, x, opt_gp, opt_sgp = plot_fig3()\n",
    "fig3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9002f",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "What does this look like? Well say we're fitting these data in black. As a standard Gaussian process fits it uses all the data available to it, whereas in Sparse Gaussian processes we fit using on a set of so-called 'inducing points' which in this case are only ten, compared to 100 in the standard GP. This low rank approximation caused by using less points speeds up the fitting of Gaussian processes enormously and allows us to fit bigger amounts of data, whilst still producing good estimates of uncertainty. You can see as well that the location of the inducing variables are not fixed and we are optimising there location as we fit the GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051c0972",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "fit_fig3(fig3, gp, sgp, x, opt_gp, opt_sgp);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909c79cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sparse hierarchical Gaussian processes\n",
    "\n",
    "\\begin{align}\n",
    "f^{(i)}(\\mathbf{x}) \\sim \\mathcal{GP}(\\mathbf{0}, k^{(i)}(\\mathbf{x}, \\mathbf{x}') + k_g(\\mathbf{x}, \\mathbf{x}'))\\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1dd357",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# Hide, run initially\n",
    "da = xr.open_dataarray('./../data/tas-ssp245.nc') - 273.15\n",
    "\n",
    "fig4 = go.Figure()\n",
    "for i in range(4):\n",
    "    fig4.add_trace(\n",
    "        go.Scatter(\n",
    "            x=da.time,\n",
    "            y=da.isel(realisation=i).values,\n",
    "            name=str(da.realisation[i].values)[:-8]\n",
    "        ))\n",
    "fig4.update_layout(\n",
    "    yaxis_title='Global mean surface temp. (°C)',\n",
    "    width=full_fig_width,\n",
    "    height=full_fig_height * 0.85,\n",
    "    margin=dict(t=5))\n",
    "fig4.show(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740503d9",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We can apply the same sparse methods to the hierarchical model we defined earlier to allow us to fit larger data that we see a lot of in the environmental sciences.\n",
    "\n",
    "Let's take a climate example. These are four surface temperature projections out to the end of the century for central England taken from ssp245 simulations which is a middle of the road emissions scenario. These are all from seperate climate models. As I said earlier the trend across all the models should on the whole be similar, as should the seasonal cycle, but there'll be variation due to the differences in the model and the inherent chaos of the Earth system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14e5dfb",
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fig5, shgp, da, norm_vals = plot_fig5()\n",
    "fig5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d186a7",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "What I'm showing here are 4 of the models on the right hand side, with their projections of surface temperature at the end of the century in the black dots. The fit of the hierarhical GP model is the coloured line, which is the mean and the shading is a 95% credible interval. This left hand side is the latent function aka the estimate of the signal that underlies all the models. These gray lines just show the means of the four models. What we see now is the prior of the model, before anything has been fit.\n",
    "\n",
    "But as I fit the model, we constrain the posterior of both the individual model fits and therefore we constrain the latent posterior. I should say that this is fitting in real time for non compiled code, so we're fitting over 100000 datapoints which is something we wouldn't be able to do at this speed with a non sparse GP model. But I won't make you sit through the whole fitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc26fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_fig5(fig5, shgp, da, norm_vals, n_iters=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c025d8a2",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Rather than watching that compile for 100+ steps this is what the final fit looks like. We've fit the more wiggly signal of each climate model whilst capturing the smoother latent trend which we interpret as the most likely model, in other words that's our best estimate of surface temperature projections for central england under the SSP245 scenario. It's still not as smooth as we would expect but that's due to the fact we've only used 4 models. When we do this with the entire the nearly 200 simulations run by CMIP6 models, the latent function is much smoother"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eae79e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What next?\n",
    "\n",
    "* Applications for air quality - averaging across low cost sensors\n",
    "* Climate model ensembling\n",
    "\n",
    "<center><img src=\"assets/GMST_scenario_comparison_with_1sd.png\" width=\"700\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39acba6e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As with all these talk, that was a whistle stop tour of how we can this scalable GP model we've developed and how we can use it in many environmental applications to model hierarchical systems and to specifically extract the latent behaviour of a system.\n",
    "\n",
    "One ongoing collaboration which we're just writing up an extension of the climate example I just gave. The GP model I've described here is used in a larger Bayesian framework to probabilistically ensemble climate models. In that framework we use scoring metrics, such as the Kernel Stien discrepancy, to determine model skill. From that we can construct the barycentre, which is essentially the probablistic of all the climate models. This is what's shown in this figure. Each coloured line is the most likely projection pathway for the major emissions scenario from CMIP6.\n",
    "\n",
    "Alongside this another application we're exploring is the to model air quality from multiple low cost sensors across a city. It's a very flexible tool and I'm definitely excited to extend it and see what else we can do with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d352ed7",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Lastly I just wanted to highlight that this is all openly avaialable code. The Bayesian ensembling package which is functioning and nearly finished allows you to probabilistically ensemble climate model output in six lines of code. That's what the figure above was produced from.  We're hpoing to release the fully functioning package by the end of the year. The more methodological Sparse hierachical GP code is also available with a couple of air quality and climate examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7604d86b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Open source\n",
    "\n",
    "* https://github.com/mattramos/bayesian_ensembling\n",
    "* https://github.com/mattramos/SparseHGP\n",
    "* Papers on arxiv shortly!\n",
    "\n",
    "### Code snippet from Bayesian ensemblings package\n",
    "```\n",
    "hist_models, fore_models = load_model_data(dir='data/gmst/ssp119')\n",
    "hist_models.fit(model=es.SHGP())\n",
    "weight_function = es.KSDWeight()\n",
    "ksd_weights = weight_function(hist_models, observations)\n",
    "ensemble_method = es.Barycentre()\n",
    "barycentre = ensemble_method(fore_models, weights_119)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a15b7c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
